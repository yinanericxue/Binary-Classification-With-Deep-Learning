# Binary Classification With Deep Learning

## After learning the softmax regression algorithm, I was confident that I could design any classification models, no matter how many labels and features are present. However, after learning about the following dataset, I was speechless.

<img width="545" alt="Screen Shot 2022-06-28 at 8 03 59 PM" src="https://user-images.githubusercontent.com/102645083/176343140-fda8190b-d58a-4eb8-8b93-be2edaa1f610.png">

## While we could easily recognize the windmill-like patterns right away, it's impossible for a model to classify it using the classic ML algorithms. I initally thought that using KNN may do the trick, but the predictions near the outliers and center would be an absolute disaster. 

## This led me to the topic of Aritifical Neural Networks, which is a series of deep learning algorithms that simulates how the human brain process information, and it's especially useful for modeling non-linear statistical data, like what we have above. Every network starts with the input layer, where the initial data enters the workflow as artifical input neurons. They are inputted directly into the first of possibly many fully-connected hidden layers, where neurons go through a nonlinear transformation by taking in a set of weights and bias, and are outputted right away or through an activation function. This process applies to every hidden layer, and the outputs from the final hidden layer is called the Output Layer, where the conclusion can be made, and it can vary from percentages for regression problems to labels for classification problems. This entire workflow called Forward-Propagation, which is the process of calculation and storage of intermediate variables for a neural network, and every layer's input is the previous layer's output. The weights and bias are updated through every iteration, and it's important that we include a loss function after Forward-Progagation to compare the predictions to the actual values. 

<img width="626" alt="3-intro-deep-neural-networks" src="https://user-images.githubusercontent.com/102645083/176347293-f65a8a75-75bd-4521-9371-04941e70a4ee.png">

## The next part of this algorithm is called Back-Propagation, and I struggled a lot with understanding it. In classic ML algorithms, we never have to deal with an enormous amount of variables, or the weights and bias that we are trying to find, and performing partial derivatives and optimization using gradient descent were quite straightforward. However, we can't do that for neural networks. If we think of an entire network as a function, every neuron / variable directly affects the final outcome. Complex network models can have millions of them, which means by just performing partial derivative on just one neuron, we need to take in account for the chain rule caused by every single other neuron. Doing such is virtually impossible, which is why Back-Propagation is so important. By using what we've already calculated using Forward-Propagation, we can find the error rate (dC/dz) for each neuron of every layer, which makes it very simple to find the partial derivatives in term of the weights and bias. In simpler words, the two propagations work simultaneously to fine-tune the weights and bias of the entire network through each iteration.

<img width="879" alt="Screen Shot 2022-06-28 at 9 27 44 PM" src="https://user-images.githubusercontent.com/102645083/176351694-baac47c0-4d72-4dd8-b895-21257f77aec4.png">

## By understanding how this algorithm works, I was able to experiment with a model which can classify that complex dataset. I first used only NumPy to come up with a "hard-coded" version of this model. To do so, I first initialized a network using my desired amount of parameters. It's important that we set it's initial weights using random values from a fixed range because we don't want to start with the same value through every iteration. After, we set up the Forward-Propagation function, which uses tanh as the first hidden layer's activation function, and sigmoid for the output layer after. We then create a loss function, which uses cross entropy because all of our labels are 0 and 1. Lastly, we setup the Back-Propogation function, which uses what we found in Forward-Progation and find-tune the two set of weights and bias. By performing these three steps 100,000, we can see that this model becomes increasingly more accurate after more trial and error.

<img width="316" alt="Screen Shot 2022-06-28 at 10 01 29 PM" src="https://user-images.githubusercontent.com/102645083/176355414-a2387ca3-a0ff-4f25-8a3b-e2b8f3012335.png">

## Since all the predicted labels are decimals between 0 to 1, it's important that we round them to which ever integer they are closer to. After doing so and plotting out the results, it's safe to say that this model is quite accurate and ignored the outliers.

<img width="540" alt="Screen Shot 2022-06-28 at 10 03 05 PM" src="https://user-images.githubusercontent.com/102645083/176356094-48dcaa34-b92c-46f5-ae98-c802bf6b7409.png">

